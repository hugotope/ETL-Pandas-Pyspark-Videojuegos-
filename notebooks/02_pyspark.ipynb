{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364a5c49",
   "metadata": {},
   "source": [
    "# 02 - Procesamiento y ETL con PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb535e",
   "metadata": {},
   "source": [
    "Este notebook replica el flujo de análisis y ETL usando PySpark y carga los resultados en `warehouse/warehouse_pyspark.db`. Ejecuta este notebook desde la raíz del proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ff998",
   "metadata": {},
   "source": [
    "## 1. Configuración Inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f077e",
   "metadata": {},
   "source": [
    "Se importa PySpark y se crea una SparkSession usando `SparkSession.builder` para procesar datos de manera distribuida. Se definen rutas relativas para el dataset CSV y la base de datos SQLite donde se guardará el Data Warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf59665c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/17 02:11:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: ../data/videogames.csv\n",
      "SQLite DB (PySpark): ../warehouse/warehouse_pyspark.db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Rutas relativas desde notebooks/ hacia data/ y warehouse/\n",
    "DATA_PATH = \"../data/videogames.csv\"\n",
    "DB_PATH = \"../warehouse/warehouse_pyspark.db\"\n",
    "DB_URL = f\"sqlite:///{DB_PATH}\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"videogames_pyspark_etl\").getOrCreate()\n",
    "\n",
    "print(f\"CSV: {DATA_PATH}\")\n",
    "print(f\"SQLite DB (PySpark): {DB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c12c0",
   "metadata": {},
   "source": [
    "## 2. Carga del Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb5025",
   "metadata": {},
   "source": [
    "Se carga el dataset de videojuegos usando Spark DataFrame con `spark.read.csv()`, habilitando `inferSchema=True` para detectar automáticamente los tipos de datos y `header=True` para usar la primera fila como encabezados. Se muestra el esquema con `printSchema()`, dimensiones y primeras filas con `show()` para verificar la carga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62178f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Dataset cargado =====\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- cost: string (nullable = true)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- popularity: string (nullable = true)\n",
      " |-- pegi: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- developer: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      " |-- engine: string (nullable = true)\n",
      " |-- award: string (nullable = true)\n",
      " |-- dlc_support: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- metascore: string (nullable = true)\n",
      " |-- user_score: string (nullable = true)\n",
      " |-- reviews: string (nullable = true)\n",
      " |-- rating_source: string (nullable = true)\n",
      " |-- copies_sold_millions: string (nullable = true)\n",
      " |-- revenue_millions_usd: string (nullable = true)\n",
      "\n",
      "Dimensiones del dataset: 10000 filas x 21 columnas\n",
      "\n",
      "Primeras 5 filas:\n",
      "+-------------------+-------+-----+--------+-----------------+----+----+---------+------------+-------+-------------+-------------+-----------+-----------+--------+---------+----------+-----------------+-------------+--------------------+--------------------+\n",
      "|               name|  genre| cost|platform|       popularity|pegi|year|developer|   publisher| region|         mode|       engine|      award|dlc_support|language|metascore|user_score|          reviews|rating_source|copies_sold_millions|revenue_millions_usd|\n",
      "+-------------------+-------+-----+--------+-----------------+----+----+---------+------------+-------+-------------+-------------+-----------+-----------+--------+---------+----------+-----------------+-------------+--------------------+--------------------+\n",
      "|Super Mario Odyssey| Action|74.45|  Mobile|               56|  7+|2011|   Capcom| Square Enix|      ?|  Multiplayer|    CryEngine|Indie Award|    Unknown|      JP|        ?|         ?|9.388708962265735|  Metacritic |               41.93|                   ?|\n",
      "|         God of War|   RPG |    0|  Mobile|                ?|  7+|2023| Rockstar|    Nintendo|Global |       Online|        Unity|          ?|          Y|      DE|     98.1|       8.4|                ?|         IGN |                1.5M|                 N/A|\n",
      "|    Persona 5 Royal|Shooter| Free|      PS|               64|  12|2020| nintendo| Square Enix|     NA|Single-player|Custom Engine|       GotY|          Y|      DE|     31.7|       2.6|                ?|          IGN|               25.08|               889.0|\n",
      "|           NBA 2K24| Puzzle|  N/A|  Mobile|972.7113240416031|  RP|2017|     Sony| Square Enix|Global |Single-player|Custom Engine|       NONE|          ?|      ES|   80/100|       N/A|                ?|   OpenCritic|                 N/A|               $500M|\n",
      "|          Overwatch|      ?| 33.4|      PC|612.6268621737502| 18+|2015| Nintendo|Bandai Namco|  NA/EU|  Multiplayer|       Custom|Indie Award|    Unknown|      IT|     36.0|       2.3|              N/A|   Metacritic|                 N/A|                 $1B|\n",
      "+-------------------+-------+-----+--------+-----------------+----+----+---------+------------+-------+-------------+-------------+-----------+-----------+--------+---------+----------+-----------------+-------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Carga del dataset con Spark\n",
    "\n",
    "raw_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)  # Indica que el archivo CSV tiene una fila de encabezados\n",
    "    .option(\"inferSchema\", True)  # Le dice a Spark que detecte automáticamente el tipo de datos\n",
    "    .csv(DATA_PATH)\n",
    ")\n",
    "\n",
    "print(\"===== Dataset cargado =====\")\n",
    "raw_df.printSchema()\n",
    "print(f\"Dimensiones del dataset: {raw_df.count()} filas x {len(raw_df.columns)} columnas\")\n",
    "print(\"\\nPrimeras 5 filas:\")\n",
    "raw_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836e085e",
   "metadata": {},
   "source": [
    "## 3. Limpieza y normalización de datos en Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83480c",
   "metadata": {},
   "source": [
    "Se eliminan duplicados con dropDuplicates(). Las columnas numéricas copies_sold_millions y revenue_millions_usd se limpian y normalizan, convirtiendo valores con M y B, manejando valores problemáticos y rellenando nulos con la media. Finalmente, las columnas categóricas se limpian y los valores faltantes o inconsistentes se reemplazan por \"Unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c53d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 2. Limpieza de columnas de ventas/ingresos =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'copies_sold_millions' limpiada y rellenada con media: 10308561.989154695\n",
      "Columna 'revenue_millions_usd' limpiada y rellenada con media: 507393576.9315999\n",
      "\n",
      "===== 3. Limpieza de columnas categóricas =====\n",
      "Columnas categóricas limpiadas correctamente\n"
     ]
    }
   ],
   "source": [
    "print(\"===== 1. Eliminación de duplicados =====\")\n",
    "initial_count = raw_df.count()\n",
    "clean_df = raw_df.dropDuplicates()\n",
    "final_count = clean_df.count()\n",
    "print(f\"Filas antes: {initial_count}, después: {final_count}\")\n",
    "\n",
    "print(\"\\n===== 2. Limpieza de columnas de ventas/ingresos =====\")\n",
    "\n",
    "numeric_target_cols = [\"copies_sold_millions\", \"revenue_millions_usd\"]\n",
    "\n",
    "for col in numeric_target_cols:\n",
    "    if col in clean_df.columns:\n",
    "\n",
    "        # Normalización del texto\n",
    "        clean_df = clean_df.withColumn(\n",
    "            col,\n",
    "            F.upper(\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(F.trim(F.col(col)), \",\", \"\"),\n",
    "                    \"\\\\$\", \"\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Conversión a numérico\n",
    "        clean_df = clean_df.withColumn(\n",
    "            col,\n",
    "            F.when(\n",
    "                (F.col(col).isNull()) | (F.col(col).isin(\"UNKNOWN\", \"?\")),\n",
    "                None\n",
    "            )\n",
    "            .when(\n",
    "                F.col(col).endswith(\"M\"),\n",
    "                F.regexp_replace(F.col(col), \"M\", \"\").cast(DoubleType()) * 1e6\n",
    "            )\n",
    "            .when(\n",
    "                F.col(col).endswith(\"B\"),\n",
    "                F.regexp_replace(F.col(col), \"B\", \"\").cast(DoubleType()) * 1e9\n",
    "            )\n",
    "            .otherwise(F.col(col).cast(DoubleType()))\n",
    "        )\n",
    "\n",
    "        # Relleno de nulos con la media\n",
    "        mean_val = clean_df.select(F.mean(col)).first()[0]\n",
    "        if mean_val is not None:\n",
    "            clean_df = clean_df.na.fill({col: mean_val})\n",
    "            print(f\"Columna '{col}' rellenada con media: {mean_val}\")\n",
    "        else:\n",
    "            clean_df = clean_df.na.fill({col: 0.0})\n",
    "            print(f\"Columna '{col}' sin valores válidos → rellenada con 0.0\")\n",
    "\n",
    "print(\"\\n===== 3. Limpieza de columnas categóricas =====\")\n",
    "\n",
    "categorical_cols = [\n",
    "    c for c in clean_df.columns\n",
    "    if c not in numeric_target_cols\n",
    "]\n",
    "\n",
    "for col in categorical_cols:\n",
    "\n",
    "    # Limpieza básica\n",
    "    clean_df = clean_df.withColumn(col, F.trim(F.col(col)))\n",
    "\n",
    "    # Normalización de valores problemáticos\n",
    "    clean_df = clean_df.withColumn(\n",
    "        col,\n",
    "        F.when(\n",
    "            (F.col(col).isNull()) |\n",
    "            (F.col(col) == \"\") |\n",
    "            (F.upper(F.col(col)).isin(\"?\", \"N/A\", \"UNKNOWN\")),\n",
    "            \"Unknown\"\n",
    "        ).otherwise(F.col(col))\n",
    "    )\n",
    "\n",
    "print(\"\\n===== Dataset después de limpieza =====\")\n",
    "clean_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24bc07a",
   "metadata": {},
   "source": [
    "## 4. Normalización de Nombres de Columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d162b",
   "metadata": {},
   "source": [
    "Se normalizan los nombres de columnas a formato snake_case (minúsculas, guiones bajos) usando `toDF()` con una lista de nombres normalizados. Esto asegura consistencia en los nombres antes de crear las dimensiones y la tabla de hechos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42bc7a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- genre: string (nullable = false)\n",
      " |-- cost: string (nullable = false)\n",
      " |-- platform: string (nullable = false)\n",
      " |-- popularity: string (nullable = false)\n",
      " |-- pegi: string (nullable = false)\n",
      " |-- year: string (nullable = false)\n",
      " |-- developer: string (nullable = false)\n",
      " |-- publisher: string (nullable = false)\n",
      " |-- region: string (nullable = false)\n",
      " |-- mode: string (nullable = false)\n",
      " |-- engine: string (nullable = false)\n",
      " |-- award: string (nullable = false)\n",
      " |-- dlc_support: string (nullable = false)\n",
      " |-- language: string (nullable = false)\n",
      " |-- metascore: string (nullable = false)\n",
      " |-- user_score: string (nullable = false)\n",
      " |-- reviews: string (nullable = false)\n",
      " |-- rating_source: string (nullable = false)\n",
      " |-- copies_sold_millions: double (nullable = false)\n",
      " |-- revenue_millions_usd: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalización de nombres de columnas\n",
    "\n",
    "normalized_cols = [\n",
    "    c.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\") for c in clean_df.columns\n",
    "]\n",
    "clean_df = clean_df.toDF(*normalized_cols)\n",
    "\n",
    "clean_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a74b945",
   "metadata": {},
   "source": [
    "## 5. Creación de Dimensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d86086",
   "metadata": {},
   "source": [
    "Se crean las dimensiones usando `distinct()` y `orderBy()` en Spark para obtener valores únicos. Se convierten temporalmente a Pandas con `toPandas()` para asignar IDs autoincrementales, y luego se recrean como DataFrames de Spark usando `spark.createDataFrame()` para los joins posteriores. Se crean: `dim_game`, `dim_platform`, `dim_developer`, `dim_publisher` y `dim_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe122d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas disponibles después de normalización:\n",
      "['name', 'genre', 'cost', 'platform', 'popularity', 'pegi', 'year', 'developer', 'publisher', 'region', 'mode', 'engine', 'award', 'dlc_support', 'language', 'metascore', 'user_score', 'reviews', 'rating_source', 'copies_sold_millions', 'revenue_millions_usd']\n",
      "\n",
      "===== Creación de dimensiones =====\n",
      "Dimensión 'dim_game' creada: 408 filas\n",
      "Dimensión 'dim_platform' creada: 9 filas\n",
      "Dimensión 'dim_developer' creada: 13 filas\n",
      "Dimensión 'dim_publisher' creada: 12 filas\n",
      "Dimensión 'dim_year' creada: 42 filas\n"
     ]
    }
   ],
   "source": [
    "cols = clean_df.columns\n",
    "required_cols = [\"name\", \"genre\", \"platform\", \"developer\", \"publisher\", \"year\"]\n",
    "for c in required_cols:\n",
    "    if c not in cols:\n",
    "        raise ValueError(f\"Columna requerida no encontrada en el CSV: {c}\")\n",
    "\n",
    "print(\"Columnas disponibles después de normalización:\")\n",
    "print(cols)\n",
    "\n",
    "print(\"\\n===== Creación de dimensiones =====\")\n",
    "\n",
    "# Dimensión: dim_game\n",
    "dim_game_pd = (\n",
    "    clean_df.select(\"name\", \"genre\")\n",
    "    .distinct()\n",
    "    .orderBy(\"name\", \"genre\")\n",
    "    .toPandas()\n",
    ")\n",
    "dim_game_pd[\"id_game\"] = range(1, len(dim_game_pd) + 1)\n",
    "dim_game = spark.createDataFrame(dim_game_pd)\n",
    "print(f\"Dimensión 'dim_game' creada: {len(dim_game_pd)} filas\")\n",
    "\n",
    "# Dimensión: dim_platform\n",
    "dim_platform_pd = clean_df.select(\"platform\").distinct().orderBy(\"platform\").toPandas()\n",
    "dim_platform_pd[\"id_platform\"] = range(1, len(dim_platform_pd) + 1)\n",
    "dim_platform = spark.createDataFrame(dim_platform_pd)\n",
    "print(f\"Dimensión 'dim_platform' creada: {len(dim_platform_pd)} filas\")\n",
    "\n",
    "# Dimensión: dim_developer\n",
    "dim_developer_pd = clean_df.select(\"developer\").distinct().orderBy(\"developer\").toPandas()\n",
    "dim_developer_pd[\"id_developer\"] = range(1, len(dim_developer_pd) + 1)\n",
    "dim_developer = spark.createDataFrame(dim_developer_pd)\n",
    "print(f\"Dimensión 'dim_developer' creada: {len(dim_developer_pd)} filas\")\n",
    "\n",
    "# Dimensión: dim_publisher\n",
    "dim_publisher_pd = clean_df.select(\"publisher\").distinct().orderBy(\"publisher\").toPandas()\n",
    "dim_publisher_pd[\"id_publisher\"] = range(1, len(dim_publisher_pd) + 1)\n",
    "dim_publisher = spark.createDataFrame(dim_publisher_pd)\n",
    "print(f\"Dimensión 'dim_publisher' creada: {len(dim_publisher_pd)} filas\")\n",
    "\n",
    "# Dimensión: dim_year\n",
    "dim_year_pd = clean_df.select(\"year\").distinct().orderBy(\"year\").toPandas()\n",
    "dim_year_pd[\"id_year\"] = range(1, len(dim_year_pd) + 1)\n",
    "dim_year = spark.createDataFrame(dim_year_pd)\n",
    "print(f\"Dimensión 'dim_year' creada: {len(dim_year_pd)} filas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f4b88",
   "metadata": {},
   "source": [
    "## 6. Construcción de la Tabla de Hechos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55517c71",
   "metadata": {},
   "source": [
    "Se construye la tabla de hechos `fact_sales` mediante joins left (`how='left'`) entre el dataset limpio y las dimensiones usando `join()`. Se seleccionan las columnas finales con IDs de las dimensiones y las métricas (`copies_sold_millions` y `revenue_millions_usd`) para completar el esquema en estrella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2af484a4-8e0c-4ac8-97de-a8a24f0a5208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabla de hechos 'fact_sales' creada: 10000 filas\n",
      "+-------+-----------+------------+------------+-------+--------------------+--------------------+\n",
      "|id_game|id_platform|id_developer|id_publisher|id_year|copies_sold_millions|revenue_millions_usd|\n",
      "+-------+-----------+------------+------------+-------+--------------------+--------------------+\n",
      "|    200|          3|           5|          10|     23|           1500000.0|               282.3|\n",
      "|    339|          4|          12|          10|     39|               34.55|               5.0E8|\n",
      "|    373|          1|           8|           9|     33|1.0308561989154695E7|               1.0E9|\n",
      "|     38|          7|           1|          10|     41|           1500000.0| 5.073935769315999E8|\n",
      "|    371|          6|           9|           6|     34|               12.79| 5.073935769315999E8|\n",
      "+-------+-----------+------------+------------+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "fact = (\n",
    "    clean_df\n",
    "    .join(dim_game, on=[\"name\", \"genre\"], how=\"left\")\n",
    "    .join(dim_platform, on=[\"platform\"], how=\"left\")\n",
    "    .join(dim_developer, on=[\"developer\"], how=\"left\")\n",
    "    .join(dim_publisher, on=[\"publisher\"], how=\"left\")\n",
    "    .join(dim_year, on=[\"year\"], how=\"left\")\n",
    ")\n",
    "\n",
    "# Columnas métricas\n",
    "value_cols = [c for c in [\"copies_sold_millions\", \"revenue_millions_usd\"] if c in fact.columns]\n",
    "if not value_cols:\n",
    "    raise ValueError(\"No se encontraron columnas métricas para construir la tabla de hechos.\")\n",
    "\n",
    "# Selección final de la tabla de hechos\n",
    "fact_sales = fact.select(\n",
    "    \"id_game\", \"id_platform\", \"id_developer\", \"id_publisher\", \"id_year\", *value_cols\n",
    ")\n",
    "\n",
    "print(f\"\\nTabla de hechos 'fact_sales' creada: {fact_sales.count()} filas\")\n",
    "fact_sales.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0660f93",
   "metadata": {},
   "source": [
    "## 7. Carga en SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a03b1",
   "metadata": {},
   "source": [
    "Se carga el Data Warehouse en SQLite directamente desde Spark usando el conector JDBC. Se utiliza `spark.write.jdbc()` para escribir los DataFrames directamente a SQLite sin necesidad de convertir a Pandas. Se crea la carpeta `warehouse` si no existe y se usa el modo `overwrite` para sobrescribir tablas existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca8764d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 1. Creación de carpeta y configuración de la base de datos =====\n",
      "Base de datos SQLite se guardará en: ../warehouse/warehouse_pyspark.db\n",
      "\n",
      "===== 2. Guardado de tablas en SQLite usando JDBC =====\n",
      "Escribiendo DataFrames de Spark directamente a SQLite...\n",
      "\n",
      "⚠️ Error al usar JDBC directo: An error occurred while calling o1681.jdbc.\n",
      ": java.lang.ClassNotFoundException: org.sqlite.JDBC\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:334)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
      "\t\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
      "\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n",
      "\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n",
      "\t\tat scala.Option.foreach(Option.scala:437)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n",
      "\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n",
      "\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n",
      "\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n",
      "\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n",
      "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\t... 20 more\n",
      "\n",
      "Nota: Si el driver JDBC de SQLite no está disponible, puedes:\n",
      "  1. Descargar sqlite-jdbc desde: https://github.com/xerial/sqlite-jdbc/releases\n",
      "  2. Agregarlo al classpath de Spark\n",
      "  3. O usar la alternativa con toPandas() (celda siguiente)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1681.jdbc.\n: java.lang.ClassNotFoundException: org.sqlite.JDBC\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:334)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n\t\tat scala.Option.foreach(Option.scala:437)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\n\t\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Cargar todas las tablas\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Dimensión: dim_game\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mwrite_to_sqlite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_game\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdim_game\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Dimensión: dim_platform\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     write_to_sqlite(dim_platform, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim_platform\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mwrite_to_sqlite\u001b[0;34m(df, table_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite_to_sqlite\u001b[39m(df, table_name):\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Escribe un DataFrame de Spark a SQLite usando JDBC\"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.sqlite.JDBC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjdbc_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjdbc_properties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cargada\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/sql/readwriter.py:2347\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m   2346\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[0;32m-> 2347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1681.jdbc.\n: java.lang.ClassNotFoundException: org.sqlite.JDBC\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:334)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n\t\tat scala.Option.foreach(Option.scala:437)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\n\t\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "print(\"===== 1. Creación de carpeta y configuración de la base de datos =====\")\n",
    "# Creamos la carpeta warehouse si no existe\n",
    "os.makedirs(\"../warehouse\", exist_ok=True)\n",
    "print(f\"Base de datos SQLite se guardará en: {DB_PATH}\")\n",
    "\n",
    "# Configuración de JDBC para SQLite\n",
    "# Nota: PySpark necesita el driver JDBC de SQLite\n",
    "# Se puede descargar desde: https://github.com/xerial/sqlite-jdbc/releases\n",
    "# O usar el que viene con algunas distribuciones de Spark\n",
    "jdbc_url = f\"jdbc:sqlite:{DB_PATH}\"\n",
    "jdbc_properties = {\n",
    "    \"driver\": \"org.sqlite.JDBC\"\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Guardado de tablas dimensionales y tabla de hechos usando Spark JDBC\n",
    "# -------------------------------\n",
    "print(\"\\n===== 2. Guardado de tablas en SQLite usando JDBC =====\")\n",
    "print(\"Escribiendo DataFrames de Spark directamente a SQLite...\")\n",
    "\n",
    "# Función auxiliar para escribir con modo overwrite\n",
    "def write_to_sqlite(df, table_name):\n",
    "    \"\"\"Escribe un DataFrame de Spark a SQLite usando JDBC\"\"\"\n",
    "    df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"driver\", \"org.sqlite.JDBC\") \\\n",
    "        .jdbc(jdbc_url, table_name, properties=jdbc_properties)\n",
    "    print(f\" - {table_name} cargada\")\n",
    "\n",
    "# Cargar todas las tablas\n",
    "try:\n",
    "    # Dimensión: dim_game\n",
    "    write_to_sqlite(dim_game, \"dim_game\")\n",
    "    \n",
    "    # Dimensión: dim_platform\n",
    "    write_to_sqlite(dim_platform, \"dim_platform\")\n",
    "    \n",
    "    # Dimensión: dim_developer\n",
    "    write_to_sqlite(dim_developer, \"dim_developer\")\n",
    "    \n",
    "    # Dimensión: dim_publisher\n",
    "    write_to_sqlite(dim_publisher, \"dim_publisher\")\n",
    "    \n",
    "    # Dimensión: dim_year\n",
    "    write_to_sqlite(dim_year, \"dim_year\")\n",
    "    \n",
    "    # Tabla de hechos: fact_sales\n",
    "    write_to_sqlite(fact_sales, \"fact_sales\")\n",
    "    \n",
    "    print(\"\\n✅ Todas las tablas fueron cargadas correctamente en SQLite usando PySpark JDBC.\")\n",
    "    print(f\"Base de datos disponible en: {DB_PATH}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Error al usar JDBC directo: {e}\")\n",
    "    print(\"Nota: Si el driver JDBC de SQLite no está disponible, puedes:\")\n",
    "    print(\"  1. Descargar sqlite-jdbc desde: https://github.com/xerial/sqlite-jdbc/releases\")\n",
    "    print(\"  2. Agregarlo al classpath de Spark\")\n",
    "    print(\"  3. O usar la alternativa con toPandas() (celda siguiente)\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07981723",
   "metadata": {},
   "source": [
    "**Nota:** Si el driver JDBC de SQLite no está disponible en tu entorno, puedes usar la alternativa con `toPandas()` en la siguiente celda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e41dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# ALTERNATIVA: Carga usando toPandas() (si JDBC no está disponible)\n",
    "# -------------------------------\n",
    "\n",
    "# Descomenta este código si el método JDBC anterior no funciona\n",
    "\n",
    "print(\"===== Alternativa: Carga usando toPandas() =====\")\n",
    "# \n",
    "# # Creamos el engine para SQLite\n",
    "engine = sqlalchemy.create_engine(DB_URL)\n",
    "# \n",
    "# # Convertir dimensiones a Pandas y cargar en SQLite\n",
    "with engine.begin() as conn:\n",
    "    dim_game.toPandas().to_sql(\"dim_game\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - dim_game cargada\")\n",
    "    dim_platform.toPandas().to_sql(\"dim_platform\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - dim_platform cargada\")\n",
    "    dim_developer.toPandas().to_sql(\"dim_developer\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - dim_developer cargada\")\n",
    "    dim_publisher.toPandas().to_sql(\"dim_publisher\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - dim_publisher cargada\")\n",
    "    dim_year.toPandas().to_sql(\"dim_year\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - dim_year cargada\")\n",
    "    fact_sales.toPandas().to_sql(\"fact_sales\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - fact_sales cargada\")\n",
    " \n",
    "print(\"\\n✅ Tablas cargadas usando método alternativo (toPandas).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85726ed",
   "metadata": {},
   "source": [
    "## 8. Consultas y Validación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b7a0c",
   "metadata": {},
   "source": [
    "Se ejecuta una consulta SQL de ejemplo para validar el Data Warehouse: se calcula el top 10 de géneros por ventas usando joins entre `fact_sales` y `dim_game`. Se muestra el resultado con las métricas de ventas e ingresos por género."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1816ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de consulta sobre el Data Warehouse generado por PySpark\n",
    "\n",
    "print(\"===== Consulta: Top 10 géneros por ventas =====\")\n",
    "\n",
    "# Opción 1: Usar Spark para leer desde SQLite (si JDBC está disponible)\n",
    "try:\n",
    "    # Leer tablas desde SQLite usando Spark JDBC\n",
    "    dim_game_sql = spark.read.jdbc(jdbc_url, \"dim_game\", properties=jdbc_properties)\n",
    "    fact_sales_sql = spark.read.jdbc(jdbc_url, \"fact_sales\", properties=jdbc_properties)\n",
    "    \n",
    "    # Realizar la consulta usando Spark SQL\n",
    "    top_genres_spark = fact_sales_sql \\\n",
    "        .join(dim_game_sql, fact_sales_sql.id_game == dim_game_sql.id_game, \"inner\") \\\n",
    "        .groupBy(\"genre\") \\\n",
    "        .agg(\n",
    "            F.sum(\"copies_sold_millions\").alias(\"total_copies_sold\"),\n",
    "            F.sum(\"revenue_millions_usd\").alias(\"total_revenue\")\n",
    "        ) \\\n",
    "        .orderBy(F.desc(\"total_copies_sold\")) \\\n",
    "        .limit(10)\n",
    "    \n",
    "    print(\"\\n===== Resultado: Top 10 géneros por ventas (usando Spark JDBC) =====\")\n",
    "    top_genres_spark.show(truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error al leer con Spark JDBC: {e}\")\n",
    "    print(\"Usando método alternativo con SQLAlchemy...\")\n",
    "    \n",
    "    # Opción 2: Usar SQLAlchemy (alternativa)\n",
    "    engine = sqlalchemy.create_engine(DB_URL)\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        g.genre, \n",
    "        SUM(f.copies_sold_millions) AS total_copies_sold,\n",
    "        SUM(f.revenue_millions_usd) AS total_revenue\n",
    "    FROM fact_sales f\n",
    "    JOIN dim_game g ON f.id_game = g.id_game\n",
    "    GROUP BY g.genre\n",
    "    ORDER BY total_copies_sold DESC\n",
    "    LIMIT 10;\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Consulta SQL ejecutada:\\n\", query)\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        top_genres_spark = pd.read_sql(query, conn)\n",
    "    \n",
    "    print(\"\\n===== Resultado: Top 10 géneros por ventas =====\")\n",
    "    print(top_genres_spark)\n",
    "\n",
    "print(\"\\n===== Resumen del Data Warehouse =====\")\n",
    "print(f\"Base de datos creada en: {DB_PATH}\")\n",
    "print(\"\\nTablas creadas:\")\n",
    "print(\"  - dim_game (dimensiones de juegos)\")\n",
    "print(\"  - dim_platform (dimensiones de plataformas)\")\n",
    "print(\"  - dim_developer (dimensiones de desarrolladores)\")\n",
    "print(\"  - dim_publisher (dimensiones de publishers)\")\n",
    "print(\"  - dim_year (dimensiones de años)\")\n",
    "print(\"  - fact_sales (tabla de hechos con ventas)\")\n",
    "print(\"\\n✅ Proceso ETL con PySpark completado exitosamente\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
