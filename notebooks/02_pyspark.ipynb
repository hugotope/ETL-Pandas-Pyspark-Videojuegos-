{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364a5c49",
   "metadata": {},
   "source": [
    "# 02 - Procesamiento y ETL con PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb535e",
   "metadata": {},
   "source": [
    "Este notebook replica el flujo de análisis y ETL usando PySpark y carga los resultados en `warehouse/warehouse_pyspark.db`. Ejecuta este notebook desde la raíz del proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ff998",
   "metadata": {},
   "source": [
    "## 1. Configuración Inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f077e",
   "metadata": {},
   "source": [
    "Se importa PySpark y se crea una SparkSession usando `SparkSession.builder` para procesar datos de manera distribuida. Se definen rutas relativas para el dataset CSV y la base de datos SQLite donde se guardará el Data Warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf59665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Rutas relativas desde notebooks/ hacia data/ y warehouse/\n",
    "DATA_PATH = \"../data/videogames.csv\"\n",
    "DB_PATH = \"../warehouse/warehouse_pyspark.db\"\n",
    "DB_URL = f\"sqlite:///{DB_PATH}\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"videogames_pyspark_etl\").getOrCreate()\n",
    "\n",
    "print(f\"CSV: {DATA_PATH}\")\n",
    "print(f\"SQLite DB (PySpark): {DB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c12c0",
   "metadata": {},
   "source": [
    "## 2. Carga del Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb5025",
   "metadata": {},
   "source": [
    "Se carga el dataset de videojuegos usando Spark DataFrame con `spark.read.csv()`, habilitando `inferSchema=True` para detectar automáticamente los tipos de datos y `header=True` para usar la primera fila como encabezados. Se muestra el esquema con `printSchema()`, dimensiones y primeras filas con `show()` para verificar la carga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62178f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del dataset con Spark\n",
    "\n",
    "raw_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)  # Indica que el archivo CSV tiene una fila de encabezados\n",
    "    .option(\"inferSchema\", True)  # Le dice a Spark que detecte automáticamente el tipo de datos\n",
    "    .csv(DATA_PATH)\n",
    ")\n",
    "\n",
    "print(\"===== Dataset cargado =====\")\n",
    "raw_df.printSchema()\n",
    "print(f\"Dimensiones del dataset: {raw_df.count()} filas x {len(raw_df.columns)} columnas\")\n",
    "print(\"\\nPrimeras 5 filas:\")\n",
    "raw_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836e085e",
   "metadata": {},
   "source": [
    "## 3. Limpieza y normalización de datos en Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83480c",
   "metadata": {},
   "source": [
    "Se eliminan duplicados con dropDuplicates(). Las columnas numéricas copies_sold_millions y revenue_millions_usd se limpian y normalizan, convirtiendo valores con M y B, manejando valores problemáticos y rellenando nulos con la media. Finalmente, las columnas categóricas se limpian y los valores faltantes o inconsistentes se reemplazan por \"Unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== 1. Eliminación de duplicados =====\")\n",
    "initial_count = raw_df.count()\n",
    "clean_df = raw_df.dropDuplicates()\n",
    "final_count = clean_df.count()\n",
    "print(f\"Filas antes: {initial_count}, después: {final_count}\")\n",
    "\n",
    "print(\"\\n===== 2. Limpieza de columnas de ventas/ingresos =====\")\n",
    "\n",
    "numeric_target_cols = [\"copies_sold_millions\", \"revenue_millions_usd\"]\n",
    "\n",
    "for col in numeric_target_cols:\n",
    "    if col in clean_df.columns:\n",
    "\n",
    "        # Normalización del texto\n",
    "        clean_df = clean_df.withColumn(\n",
    "            col,\n",
    "            F.upper(\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(F.trim(F.col(col)), \",\", \"\"),\n",
    "                    \"\\\\$\", \"\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Conversión a numérico\n",
    "        clean_df = clean_df.withColumn(\n",
    "            col,\n",
    "            F.when(\n",
    "                (F.col(col).isNull()) | (F.col(col).isin(\"UNKNOWN\", \"?\")),\n",
    "                None\n",
    "            )\n",
    "            .when(\n",
    "                F.col(col).endswith(\"M\"),\n",
    "                F.regexp_replace(F.col(col), \"M\", \"\").cast(DoubleType()) * 1e6\n",
    "            )\n",
    "            .when(\n",
    "                F.col(col).endswith(\"B\"),\n",
    "                F.regexp_replace(F.col(col), \"B\", \"\").cast(DoubleType()) * 1e9\n",
    "            )\n",
    "            .otherwise(F.col(col).cast(DoubleType()))\n",
    "        )\n",
    "\n",
    "        # Relleno de nulos con la media\n",
    "        mean_val = clean_df.select(F.mean(col)).first()[0]\n",
    "        if mean_val is not None:\n",
    "            clean_df = clean_df.na.fill({col: mean_val})\n",
    "            print(f\"Columna '{col}' rellenada con media: {mean_val}\")\n",
    "        else:\n",
    "            clean_df = clean_df.na.fill({col: 0.0})\n",
    "            print(f\"Columna '{col}' sin valores válidos → rellenada con 0.0\")\n",
    "\n",
    "print(\"\\n===== 3. Limpieza de columnas categóricas =====\")\n",
    "\n",
    "categorical_cols = [\n",
    "    c for c in clean_df.columns\n",
    "    if c not in numeric_target_cols\n",
    "]\n",
    "\n",
    "for col in categorical_cols:\n",
    "\n",
    "    # Limpieza básica\n",
    "    clean_df = clean_df.withColumn(col, F.trim(F.col(col)))\n",
    "\n",
    "    # Normalización de valores problemáticos\n",
    "    clean_df = clean_df.withColumn(\n",
    "        col,\n",
    "        F.when(\n",
    "            (F.col(col).isNull()) |\n",
    "            (F.col(col) == \"\") |\n",
    "            (F.upper(F.col(col)).isin(\"?\", \"N/A\", \"UNKNOWN\")),\n",
    "            \"Unknown\"\n",
    "        ).otherwise(F.col(col))\n",
    "    )\n",
    "\n",
    "print(\"\\n===== Dataset después de limpieza =====\")\n",
    "clean_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24bc07a",
   "metadata": {},
   "source": [
    "## 4. Normalización de Nombres de Columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d162b",
   "metadata": {},
   "source": [
    "Se normalizan los nombres de columnas a formato snake_case (minúsculas, guiones bajos) usando `toDF()` con una lista de nombres normalizados. Esto asegura consistencia en los nombres antes de crear las dimensiones y la tabla de hechos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc7a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización de nombres de columnas\n",
    "\n",
    "normalized_cols = [\n",
    "    c.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\") for c in clean_df.columns\n",
    "]\n",
    "clean_df = clean_df.toDF(*normalized_cols)\n",
    "\n",
    "clean_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a74b945",
   "metadata": {},
   "source": [
    "## 5. Creación de Dimensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d86086",
   "metadata": {},
   "source": [
    "Se crean las dimensiones usando `distinct()` y `orderBy()` en Spark para obtener valores únicos. Se convierten temporalmente a Pandas con `toPandas()` para asignar IDs autoincrementales, y luego se recrean como DataFrames de Spark usando `spark.createDataFrame()` para los joins posteriores. Se crean: `dim_game`, `dim_platform`, `dim_developer`, `dim_publisher` y `dim_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe122d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = clean_df.columns\n",
    "required_cols = [\"name\", \"genre\", \"platform\", \"developer\", \"publisher\", \"year\"]\n",
    "for c in required_cols:\n",
    "    if c not in cols:\n",
    "        raise ValueError(f\"Columna requerida no encontrada en el CSV: {c}\")\n",
    "\n",
    "print(\"Columnas disponibles después de normalización:\")\n",
    "print(cols)\n",
    "\n",
    "print(\"\\n===== Creación de dimensiones =====\")\n",
    "\n",
    "# Dimensión: dim_game\n",
    "dim_game_pd = (\n",
    "    clean_df.select(\"name\", \"genre\")\n",
    "    .distinct()\n",
    "    .orderBy(\"name\", \"genre\")\n",
    "    .toPandas()\n",
    ")\n",
    "dim_game_pd[\"id_game\"] = range(1, len(dim_game_pd) + 1)\n",
    "dim_game = spark.createDataFrame(dim_game_pd)\n",
    "print(f\"Dimensión 'dim_game' creada: {len(dim_game_pd)} filas\")\n",
    "\n",
    "# Dimensión: dim_platform\n",
    "dim_platform_pd = clean_df.select(\"platform\").distinct().orderBy(\"platform\").toPandas()\n",
    "dim_platform_pd[\"id_platform\"] = range(1, len(dim_platform_pd) + 1)\n",
    "dim_platform = spark.createDataFrame(dim_platform_pd)\n",
    "print(f\"Dimensión 'dim_platform' creada: {len(dim_platform_pd)} filas\")\n",
    "\n",
    "# Dimensión: dim_developer\n",
    "dim_developer_pd = clean_df.select(\"developer\").distinct().orderBy(\"developer\").toPandas()\n",
    "dim_developer_pd[\"id_developer\"] = range(1, len(dim_developer_pd) + 1)\n",
    "dim_developer = spark.createDataFrame(dim_developer_pd)\n",
    "print(f\"Dimensión 'dim_developer' creada: {len(dim_developer_pd)} filas\")\n",
    "\n",
    "# Dimensión: dim_publisher\n",
    "dim_publisher_pd = clean_df.select(\"publisher\").distinct().orderBy(\"publisher\").toPandas()\n",
    "dim_publisher_pd[\"id_publisher\"] = range(1, len(dim_publisher_pd) + 1)\n",
    "dim_publisher = spark.createDataFrame(dim_publisher_pd)\n",
    "print(f\"Dimensión 'dim_publisher' creada: {len(dim_publisher_pd)} filas\")\n",
    "\n",
    "# Dimensión: dim_year\n",
    "dim_year_pd = clean_df.select(\"year\").distinct().orderBy(\"year\").toPandas()\n",
    "dim_year_pd[\"id_year\"] = range(1, len(dim_year_pd) + 1)\n",
    "dim_year = spark.createDataFrame(dim_year_pd)\n",
    "print(f\"Dimensión 'dim_year' creada: {len(dim_year_pd)} filas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f4b88",
   "metadata": {},
   "source": [
    "## 6. Construcción de la Tabla de Hechos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55517c71",
   "metadata": {},
   "source": [
    "Se construye la tabla de hechos `fact_sales` mediante joins left (`how='left'`) entre el dataset limpio y las dimensiones usando `join()`. Se seleccionan las columnas finales con IDs de las dimensiones y las métricas (`copies_sold_millions` y `revenue_millions_usd`) para completar el esquema en estrella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af484a4-8e0c-4ac8-97de-a8a24f0a5208",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact = (\n",
    "    clean_df\n",
    "    .join(dim_game, on=[\"name\", \"genre\"], how=\"left\")\n",
    "    .join(dim_platform, on=[\"platform\"], how=\"left\")\n",
    "    .join(dim_developer, on=[\"developer\"], how=\"left\")\n",
    "    .join(dim_publisher, on=[\"publisher\"], how=\"left\")\n",
    "    .join(dim_year, on=[\"year\"], how=\"left\")\n",
    ")\n",
    "\n",
    "# Columnas métricas\n",
    "value_cols = [c for c in [\"copies_sold_millions\", \"revenue_millions_usd\"] if c in fact.columns]\n",
    "if not value_cols:\n",
    "    raise ValueError(\"No se encontraron columnas métricas para construir la tabla de hechos.\")\n",
    "\n",
    "# Selección final de la tabla de hechos\n",
    "fact_sales = fact.select(\n",
    "    \"id_game\", \"id_platform\", \"id_developer\", \"id_publisher\", \"id_year\", *value_cols\n",
    ")\n",
    "\n",
    "print(f\"\\nTabla de hechos 'fact_sales' creada: {fact_sales.count()} filas\")\n",
    "fact_sales.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0660f93",
   "metadata": {},
   "source": [
    "## 7. Carga en SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a03b1",
   "metadata": {},
   "source": [
    "Se carga el Data Warehouse en SQLite directamente desde Spark usando el conector JDBC. Se utiliza `spark.write.jdbc()` para escribir los DataFrames directamente a SQLite sin necesidad de convertir a Pandas. Se crea la carpeta `warehouse` si no existe y se usa el modo `overwrite` para sobrescribir tablas existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8764d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== 1. Creación de carpeta y configuración de la base de datos =====\")\n",
    "# Creamos la carpeta warehouse si no existe\n",
    "os.makedirs(\"../warehouse\", exist_ok=True)\n",
    "print(f\"Base de datos SQLite se guardará en: {DB_PATH}\")\n",
    "\n",
    "# Configuración de JDBC para SQLite\n",
    "# Nota: PySpark necesita el driver JDBC de SQLite\n",
    "# Se puede descargar desde: https://github.com/xerial/sqlite-jdbc/releases\n",
    "# O usar el que viene con algunas distribuciones de Spark\n",
    "jdbc_url = f\"jdbc:sqlite:{DB_PATH}\"\n",
    "jdbc_properties = {\n",
    "    \"driver\": \"org.sqlite.JDBC\"\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Guardado de tablas dimensionales y tabla de hechos usando Spark JDBC\n",
    "# -------------------------------\n",
    "print(\"\\n===== 2. Guardado de tablas en SQLite usando JDBC =====\")\n",
    "print(\"Escribiendo DataFrames de Spark directamente a SQLite...\")\n",
    "\n",
    "# Función auxiliar para escribir con modo overwrite\n",
    "def write_to_sqlite(df, table_name):\n",
    "    \"\"\"Escribe un DataFrame de Spark a SQLite usando JDBC\"\"\"\n",
    "    df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"driver\", \"org.sqlite.JDBC\") \\\n",
    "        .jdbc(jdbc_url, table_name, properties=jdbc_properties)\n",
    "    print(f\" - {table_name} cargada\")\n",
    "\n",
    "# Cargar todas las tablas\n",
    "try:\n",
    "    # Dimensión: dim_game\n",
    "    write_to_sqlite(dim_game, \"dim_game\")\n",
    "    \n",
    "    # Dimensión: dim_platform\n",
    "    write_to_sqlite(dim_platform, \"dim_platform\")\n",
    "    \n",
    "    # Dimensión: dim_developer\n",
    "    write_to_sqlite(dim_developer, \"dim_developer\")\n",
    "    \n",
    "    # Dimensión: dim_publisher\n",
    "    write_to_sqlite(dim_publisher, \"dim_publisher\")\n",
    "    \n",
    "    # Dimensión: dim_year\n",
    "    write_to_sqlite(dim_year, \"dim_year\")\n",
    "    \n",
    "    # Tabla de hechos: fact_sales\n",
    "    write_to_sqlite(fact_sales, \"fact_sales\")\n",
    "    \n",
    "    print(\"\\n✅ Todas las tablas fueron cargadas correctamente en SQLite usando PySpark JDBC.\")\n",
    "    print(f\"Base de datos disponible en: {DB_PATH}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Error al usar JDBC directo: {e}\")\n",
    "    print(\"Nota: Si el driver JDBC de SQLite no está disponible, puedes:\")\n",
    "    print(\"  1. Descargar sqlite-jdbc desde: https://github.com/xerial/sqlite-jdbc/releases\")\n",
    "    print(\"  2. Agregarlo al classpath de Spark\")\n",
    "    print(\"  3. O usar la alternativa con toPandas() (celda siguiente)\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07981723",
   "metadata": {},
   "source": [
    "**Nota:** Si el driver JDBC de SQLite no está disponible en tu entorno, puedes usar la alternativa con `toPandas()` en la siguiente celda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e41dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# ALTERNATIVA: Carga usando toPandas() (si JDBC no está disponible)\n",
    "# -------------------------------\n",
    "\n",
    "# Descomenta este código si el método JDBC anterior no funciona\n",
    "\n",
    "print(\"===== Alternativa: Carga usando toPandas() =====\")\n",
    "# \n",
    "# # Creamos el engine para SQLite\n",
    "engine = sqlalchemy.create_engine(DB_URL)\n",
    "# \n",
    "# # Convertir dimensiones a Pandas y cargar en SQLite\n",
    "with engine.begin() as conn:\n",
    "    dim_game.toPandas().to_sql(\"dim_game\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - dim_game cargada\")\n",
    "    dim_platform.toPandas().to_sql(\"dim_platform\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - dim_platform cargada\")\n",
    "    dim_developer.toPandas().to_sql(\"dim_developer\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - dim_developer cargada\")\n",
    "    dim_publisher.toPandas().to_sql(\"dim_publisher\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - dim_publisher cargada\")\n",
    "    dim_year.toPandas().to_sql(\"dim_year\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - dim_year cargada\")\n",
    "    fact_sales.toPandas().to_sql(\"fact_sales\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - fact_sales cargada\")\n",
    " \n",
    "print(\"\\n✅ Tablas cargadas usando método alternativo (toPandas).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85726ed",
   "metadata": {},
   "source": [
    "## 8. Consultas y Validación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b7a0c",
   "metadata": {},
   "source": [
    "Se ejecuta una consulta SQL de ejemplo para validar el Data Warehouse: se calcula el top 10 de géneros por ventas usando joins entre `fact_sales` y `dim_game`. Se muestra el resultado con las métricas de ventas e ingresos por género."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1816ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de consulta sobre el Data Warehouse generado por PySpark\n",
    "\n",
    "print(\"===== Consulta: Top 10 géneros por ventas =====\")\n",
    "\n",
    "# Opción 1: Usar Spark para leer desde SQLite (si JDBC está disponible)\n",
    "try:\n",
    "    # Leer tablas desde SQLite usando Spark JDBC\n",
    "    dim_game_sql = spark.read.jdbc(jdbc_url, \"dim_game\", properties=jdbc_properties)\n",
    "    fact_sales_sql = spark.read.jdbc(jdbc_url, \"fact_sales\", properties=jdbc_properties)\n",
    "    \n",
    "    # Realizar la consulta usando Spark SQL\n",
    "    top_genres_spark = fact_sales_sql \\\n",
    "        .join(dim_game_sql, fact_sales_sql.id_game == dim_game_sql.id_game, \"inner\") \\\n",
    "        .groupBy(\"genre\") \\\n",
    "        .agg(\n",
    "            F.sum(\"copies_sold_millions\").alias(\"total_copies_sold\"),\n",
    "            F.sum(\"revenue_millions_usd\").alias(\"total_revenue\")\n",
    "        ) \\\n",
    "        .orderBy(F.desc(\"total_copies_sold\")) \\\n",
    "        .limit(10)\n",
    "    \n",
    "    print(\"\\n===== Resultado: Top 10 géneros por ventas (usando Spark JDBC) =====\")\n",
    "    top_genres_spark.show(truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error al leer con Spark JDBC: {e}\")\n",
    "    print(\"Usando método alternativo con SQLAlchemy...\")\n",
    "    \n",
    "    # Opción 2: Usar SQLAlchemy (alternativa)\n",
    "    engine = sqlalchemy.create_engine(DB_URL)\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        g.genre, \n",
    "        SUM(f.copies_sold_millions) AS total_copies_sold,\n",
    "        SUM(f.revenue_millions_usd) AS total_revenue\n",
    "    FROM fact_sales f\n",
    "    JOIN dim_game g ON f.id_game = g.id_game\n",
    "    GROUP BY g.genre\n",
    "    ORDER BY total_copies_sold DESC\n",
    "    LIMIT 10;\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Consulta SQL ejecutada:\\n\", query)\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        top_genres_spark = pd.read_sql(query, conn)\n",
    "    \n",
    "    print(\"\\n===== Resultado: Top 10 géneros por ventas =====\")\n",
    "    print(top_genres_spark)\n",
    "\n",
    "print(\"\\n===== Resumen del Data Warehouse =====\")\n",
    "print(f\"Base de datos creada en: {DB_PATH}\")\n",
    "print(\"\\nTablas creadas:\")\n",
    "print(\"  - dim_game (dimensiones de juegos)\")\n",
    "print(\"  - dim_platform (dimensiones de plataformas)\")\n",
    "print(\"  - dim_developer (dimensiones de desarrolladores)\")\n",
    "print(\"  - dim_publisher (dimensiones de publishers)\")\n",
    "print(\"  - dim_year (dimensiones de años)\")\n",
    "print(\"  - fact_sales (tabla de hechos con ventas)\")\n",
    "print(\"\\n✅ Proceso ETL con PySpark completado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3d9b53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e4cc28b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
