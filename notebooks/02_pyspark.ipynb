{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1900044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - Procesamiento y ETL con PySpark\n",
    "\n",
    "# Este notebook replica el flujo de análisis y ETL usando PySpark y carga los resultados en `warehouse/warehouse_pyspark.db`. Ejecuta este notebook desde la raíz del proyecto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ff998",
   "metadata": {},
   "source": [
    "# 1. Configuración Inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f077e",
   "metadata": {},
   "source": [
    "Se importa PySpark y se crea una SparkSession para procesar datos de manera distribuida. Se definen rutas relativas para el dataset y la base de datos SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf59665c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: ../data/videogames.csv\n",
      "SQLite DB (PySpark): ../warehouse/warehouse_pyspark.db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "\n",
    "# Rutas relativas desde notebooks/ hacia data/ y warehouse/\n",
    "DATA_PATH = \"../data/videogames.csv\"\n",
    "DB_PATH = \"../warehouse/warehouse_pyspark.db\"\n",
    "DB_URL = f\"sqlite:///{DB_PATH}\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"videogames_pyspark_etl\").getOrCreate()\n",
    "\n",
    "print(f\"CSV: {DATA_PATH}\")\n",
    "print(f\"SQLite DB (PySpark): {DB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c12c0",
   "metadata": {},
   "source": [
    "# 2. Carga del Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb5025",
   "metadata": {},
   "source": [
    "Se carga el dataset de videojuegos usando Spark DataFrame con spark.read.csv, habilitando inferencia de esquema. Se muestra el esquema, dimensiones y primeras filas para verificar la carga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62178f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Dataset cargado =====\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- cost: string (nullable = true)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- popularity: string (nullable = true)\n",
      " |-- pegi: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- developer: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      " |-- engine: string (nullable = true)\n",
      " |-- award: string (nullable = true)\n",
      " |-- dlc_support: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- metascore: string (nullable = true)\n",
      " |-- user_score: string (nullable = true)\n",
      " |-- reviews: string (nullable = true)\n",
      " |-- rating_source: string (nullable = true)\n",
      " |-- copies_sold_millions: string (nullable = true)\n",
      " |-- revenue_millions_usd: string (nullable = true)\n",
      "\n",
      "Dimensiones del dataset: 10000 filas x 21 columnas\n",
      "\n",
      "Primeras 5 filas:\n",
      "+-------------------+-------+-----+--------+-----------------+----+----+---------+------------+-------+-------------+-------------+-----------+-----------+--------+---------+----------+-----------------+-------------+--------------------+--------------------+\n",
      "|               name|  genre| cost|platform|       popularity|pegi|year|developer|   publisher| region|         mode|       engine|      award|dlc_support|language|metascore|user_score|          reviews|rating_source|copies_sold_millions|revenue_millions_usd|\n",
      "+-------------------+-------+-----+--------+-----------------+----+----+---------+------------+-------+-------------+-------------+-----------+-----------+--------+---------+----------+-----------------+-------------+--------------------+--------------------+\n",
      "|Super Mario Odyssey| Action|74.45|  Mobile|               56|  7+|2011|   Capcom| Square Enix|      ?|  Multiplayer|    CryEngine|Indie Award|    Unknown|      JP|        ?|         ?|9.388708962265735|  Metacritic |               41.93|                   ?|\n",
      "|         God of War|   RPG |    0|  Mobile|                ?|  7+|2023| Rockstar|    Nintendo|Global |       Online|        Unity|          ?|          Y|      DE|     98.1|       8.4|                ?|         IGN |                1.5M|                 N/A|\n",
      "|    Persona 5 Royal|Shooter| Free|      PS|               64|  12|2020| nintendo| Square Enix|     NA|Single-player|Custom Engine|       GotY|          Y|      DE|     31.7|       2.6|                ?|          IGN|               25.08|               889.0|\n",
      "|           NBA 2K24| Puzzle|  N/A|  Mobile|972.7113240416031|  RP|2017|     Sony| Square Enix|Global |Single-player|Custom Engine|       NONE|          ?|      ES|   80/100|       N/A|                ?|   OpenCritic|                 N/A|               $500M|\n",
      "|          Overwatch|      ?| 33.4|      PC|612.6268621737502| 18+|2015| Nintendo|Bandai Namco|  NA/EU|  Multiplayer|       Custom|Indie Award|    Unknown|      IT|     36.0|       2.3|              N/A|   Metacritic|                 N/A|                 $1B|\n",
      "+-------------------+-------+-----+--------+-----------------+----+----+---------+------------+-------+-------------+-------------+-----------+-----------+--------+---------+----------+-----------------+-------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Carga del dataset con Spark\n",
    "\n",
    "raw_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)  # Indica que el archivo CSV tiene una fila de encabezados\n",
    "    .option(\"inferSchema\", True)  # Le dice a Spark que detecte automáticamente el tipo de datos\n",
    "    .csv(DATA_PATH)\n",
    ")\n",
    "\n",
    "print(\"===== Dataset cargado =====\")\n",
    "raw_df.printSchema()\n",
    "print(f\"Dimensiones del dataset: {raw_df.count()} filas x {len(raw_df.columns)} columnas\")\n",
    "print(\"\\nPrimeras 5 filas:\")\n",
    "raw_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9cb19",
   "metadata": {},
   "source": [
    "# 3. Limpieza de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1992650c",
   "metadata": {},
   "source": [
    "Se eliminan duplicados con dropDuplicates y se tratan nulos: imputación con media para numéricas y \"Unknown\" para categóricas. Se convierten formatos especiales en columnas numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47d90383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 1. Eliminación de duplicados =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas antes: 10000, después: 10000\n",
      "\n",
      "Columnas numéricas detectadas: []\n",
      "Columnas categóricas detectadas: 21 columnas\n",
      "\n",
      "===== 2. Tratamiento de valores faltantes en columnas categóricas =====\n",
      "Valores faltantes y problemáticos reemplazados con 'Unknown'\n",
      "\n",
      "===== Dataset después de limpieza inicial =====\n",
      "+--------------------+----------+-------+--------+------------------+-------+----+----------+-----------+------+-----------+-------------+---------+-----------+--------+---------+----------+-----------------+-------------+--------------------+--------------------+\n",
      "|                name|     genre|   cost|platform|        popularity|   pegi|year| developer|  publisher|region|       mode|       engine|    award|dlc_support|language|metascore|user_score|          reviews|rating_source|copies_sold_millions|revenue_millions_usd|\n",
      "+--------------------+----------+-------+--------+------------------+-------+----+----------+-----------+------+-----------+-------------+---------+-----------+--------+---------+----------+-----------------+-------------+--------------------+--------------------+\n",
      "|       Halo Infinite|Simulation|    $60|      PC|           Unknown|     12|2007|        EA|Square Enix|    EU|    Single |Unreal Engine|Nominated|         No|      EN|   80/100|   Unknown|             9513|      Unknown|                1.5M|               282.3|\n",
      "|          The Sims 4|     Indie|  55.44|      PS|           Unknown|      3|2023|   Unknown|Square Enix|    EU|      Co-op|Unreal Engine|Nominated|         No|      FR|   80/100|   Unknown|          Unknown|         IGN |               34.55|               $500M|\n",
      "|        Unknown Game|    Action|    $46|  Mobile|           Unknown|     7+|2017|  Rockstar|       Sony| NA/EU|Multiplayer|      Unknown|Nominated|         No|      DE|  Unknown|      9/10|          Unknown|         IGN |             Unknown|                 $1B|\n",
      "|Call of Duty: Mod...| Adventure|Unknown|    XBOX|           Unknown|unknown|9999|Activision|Square Enix|Global|     Online|      Unknown|     GotY|          Y|      IT|     30.9|   Unknown|63561.48529850485|   Metacritic|                1.5M|             Unknown|\n",
      "|                    |   Unknown| 101.51| Switch |470.25680871913966|    18+|2018|      Sony|  Microsoft|    JP|      Co-op|        Unity|  Unknown|        Yes|      FR|  Unknown|       4.8|72386.67679635916|  Metacritic |               12.79|             Unknown|\n",
      "+--------------------+----------+-------+--------+------------------+-------+----+----------+-----------+------+-----------+-------------+---------+-----------+--------+---------+----------+-----------------+-------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Tratamiento de nulos y duplicados en Spark\n",
    "\n",
    "print(\"===== 1. Eliminación de duplicados =====\")\n",
    "initial_count = raw_df.count()\n",
    "clean_df = raw_df.dropDuplicates()\n",
    "final_count = clean_df.count()\n",
    "print(f\"Filas antes: {initial_count}, después: {final_count}\")\n",
    "\n",
    "# Identificamos columnas numéricas y categóricas\n",
    "# Nota: Como inferSchema=True detecta todo como string, no habrá columnas numéricas aquí\n",
    "# Las convertiremos después en la siguiente celda\n",
    "numeric_cols = [f.name for f in clean_df.schema.fields if str(f.dataType) in (\"IntegerType\", \"LongType\", \"DoubleType\", \"FloatType\")] \n",
    "cat_cols = [f.name for f in clean_df.schema.fields if f.name not in numeric_cols]\n",
    "\n",
    "print(f\"\\nColumnas numéricas detectadas: {numeric_cols}\")\n",
    "print(f\"Columnas categóricas detectadas: {len(cat_cols)} columnas\")\n",
    "\n",
    "# Tratamiento de nulos en columnas numéricas (si las hay)\n",
    "if numeric_cols:\n",
    "    for col in numeric_cols:\n",
    "        mean_val = clean_df.select(F.mean(F.col(col))).first()[0]\n",
    "        if mean_val is not None:\n",
    "            clean_df = clean_df.na.fill({col: float(mean_val)})\n",
    "            print(f\"Columna '{col}' rellenada con media: {mean_val}\")\n",
    "        else:\n",
    "            clean_df = clean_df.na.fill({col: 0.0})\n",
    "            print(f\"Columna '{col}' rellenada con 0.0 (sin valores válidos)\")\n",
    "\n",
    "# Tratamiento de nulos en columnas categóricas\n",
    "# También tratamos valores problemáticos como \"?\", \"N/A\", etc.\n",
    "print(\"\\n===== 2. Tratamiento de valores faltantes en columnas categóricas =====\")\n",
    "for col in cat_cols:\n",
    "    # Reemplazar valores problemáticos comunes con \"Unknown\"\n",
    "    clean_df = clean_df.withColumn(\n",
    "        col,\n",
    "        F.when(\n",
    "            (F.col(col).isNull()) | \n",
    "            (F.col(col) == \"?\") | \n",
    "            (F.col(col) == \"N/A\") |\n",
    "            (F.col(col) == \"\"),\n",
    "            \"Unknown\"\n",
    "        ).otherwise(F.col(col))\n",
    "    )\n",
    "\n",
    "print(\"Valores faltantes y problemáticos reemplazados con 'Unknown'\")\n",
    "\n",
    "print(\"\\n===== Dataset después de limpieza inicial =====\")\n",
    "clean_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836e085e",
   "metadata": {},
   "source": [
    "# 4. Normalización de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83480c",
   "metadata": {},
   "source": [
    "Se normalizan nombres de columnas a snake_case y se convierten tipos de datos. Se limpian valores problemáticos en categóricas y se preparan los datos para el modelo dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d9c53d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 2. Limpieza de columnas de ventas/ingresos =====\n",
      "Columna 'copies_sold_millions' limpiada y rellenada con media: 10308561.989154695\n",
      "Columna 'revenue_millions_usd' limpiada y rellenada con media: 507393576.9315999\n",
      "\n",
      "===== 3. Limpieza de columnas categóricas =====\n",
      "Columnas categóricas limpiadas correctamente\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "print(\"\\n===== 2. Limpieza de columnas de ventas/ingresos =====\")\n",
    "\n",
    "numeric_target_cols = [\"copies_sold_millions\", \"revenue_millions_usd\"]\n",
    "\n",
    "for col in numeric_target_cols:\n",
    "    if col in clean_df.columns:\n",
    "\n",
    "        # 1️ Normalizamos el texto:\n",
    "        # - quitamos espacios\n",
    "        # - quitamos comas y $\n",
    "        # - pasamos a mayúsculas\n",
    "        clean_df = clean_df.withColumn(\n",
    "            col,\n",
    "            F.upper(\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(F.trim(F.col(col)), \",\", \"\"),\n",
    "                    \"\\\\$\", \"\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 2️ Convertimos valores especiales a NULL\n",
    "        clean_df = clean_df.withColumn(\n",
    "            col,\n",
    "            F.when(\n",
    "                (F.col(col).isNull()) |\n",
    "                (F.col(col).isin(\"UNKNOWN\", \"?\")),\n",
    "                None\n",
    "            )\n",
    "            # 3️ Valores terminados en M → millones\n",
    "            .when(\n",
    "                F.col(col).endswith(\"M\"),\n",
    "                F.regexp_replace(F.col(col), \"M\", \"\").cast(DoubleType()) * 1e6\n",
    "            )\n",
    "            # 4️ Valores terminados en B → billones\n",
    "            .when(\n",
    "                F.col(col).endswith(\"B\"),\n",
    "                F.regexp_replace(F.col(col), \"B\", \"\").cast(DoubleType()) * 1e9\n",
    "            )\n",
    "            # 5️ Intento directo de conversión a número\n",
    "            .otherwise(F.col(col).cast(DoubleType()))\n",
    "        )\n",
    "\n",
    "        # 6️ Calculamos la media de la columna\n",
    "        mean_val = clean_df.select(F.mean(col)).first()[0]\n",
    "\n",
    "        # 7️ Rellenamos los NULL con la media\n",
    "        if mean_val is not None:\n",
    "            clean_df = clean_df.na.fill({col: mean_val})\n",
    "            print(f\"Columna '{col}' limpiada y rellenada con media: {mean_val}\")\n",
    "        else:\n",
    "            clean_df = clean_df.na.fill({col: 0.0})\n",
    "            print(f\"Columna '{col}' sin valores válidos → rellenada con 0.0\")\n",
    "\n",
    "print(\"\\n===== 3. Limpieza de columnas categóricas =====\")\n",
    "\n",
    "# Columnas categóricas = todas menos las numéricas tratadas antes\n",
    "categorical_cols = [\n",
    "    c for c in clean_df.columns\n",
    "    if c not in [\"copies_sold_millions\", \"revenue_millions_usd\"]\n",
    "]\n",
    "\n",
    "for col in categorical_cols:\n",
    "\n",
    "    # 1️ Limpiamos espacios y normalizamos texto\n",
    "    clean_df = clean_df.withColumn(\n",
    "        col,\n",
    "        F.trim(F.col(col))\n",
    "    )\n",
    "\n",
    "    # 2️ Reemplazamos valores problemáticos por NULL\n",
    "    clean_df = clean_df.withColumn(\n",
    "        col,\n",
    "        F.when(\n",
    "            (F.col(col).isNull()) |\n",
    "            (F.col(col) == \"\") |\n",
    "            (F.upper(F.col(col)).isin(\"?\", \"N/A\", \"UNKNOWN\")),\n",
    "            None\n",
    "        ).otherwise(F.col(col))\n",
    "    )\n",
    "\n",
    "    # 3️ Rellenamos los NULL con 'Unknown'\n",
    "    clean_df = clean_df.na.fill({col: \"Unknown\"})\n",
    "\n",
    "print(\"Columnas categóricas limpiadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24bc07a",
   "metadata": {},
   "source": [
    "# 5. Creación de Dimensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d162b",
   "metadata": {},
   "source": [
    "Se crean las dimensiones usando distinct() y orderBy() en Spark. Se convierten a Pandas para asignar IDs y se recrean como DataFrames de Spark para los joins posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42bc7a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- genre: string (nullable = false)\n",
      " |-- cost: string (nullable = false)\n",
      " |-- platform: string (nullable = false)\n",
      " |-- popularity: string (nullable = false)\n",
      " |-- pegi: string (nullable = false)\n",
      " |-- year: string (nullable = false)\n",
      " |-- developer: string (nullable = false)\n",
      " |-- publisher: string (nullable = false)\n",
      " |-- region: string (nullable = false)\n",
      " |-- mode: string (nullable = false)\n",
      " |-- engine: string (nullable = false)\n",
      " |-- award: string (nullable = false)\n",
      " |-- dlc_support: string (nullable = false)\n",
      " |-- language: string (nullable = false)\n",
      " |-- metascore: string (nullable = false)\n",
      " |-- user_score: string (nullable = false)\n",
      " |-- reviews: string (nullable = false)\n",
      " |-- rating_source: string (nullable = false)\n",
      " |-- copies_sold_millions: double (nullable = false)\n",
      " |-- revenue_millions_usd: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalización de nombres de columnas\n",
    "\n",
    "normalized_cols = [\n",
    "    c.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\") for c in clean_df.columns\n",
    "]\n",
    "clean_df = clean_df.toDF(*normalized_cols)\n",
    "\n",
    "clean_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a74b945",
   "metadata": {},
   "source": [
    "# 6. Construcción de la Tabla de Hechos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d86086",
   "metadata": {},
   "source": [
    "Se construye la tabla de hechos mediante joins left entre el dataset y las dimensiones. Se seleccionan las columnas finales con IDs y métricas para completar el esquema en estrella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe122d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas disponibles después de normalización:\n",
      "['name', 'genre', 'cost', 'platform', 'popularity', 'pegi', 'year', 'developer', 'publisher', 'region', 'mode', 'engine', 'award', 'dlc_support', 'language', 'metascore', 'user_score', 'reviews', 'rating_source', 'copies_sold_millions', 'revenue_millions_usd']\n",
      "\n",
      "===== Creación de dimensiones =====\n",
      "Dimensión 'dim_game' creada: 408 filas\n",
      "Dimensión 'dim_platform' creada: 9 filas\n",
      "Dimensión 'dim_developer' creada: 13 filas\n",
      "Dimensión 'dim_publisher' creada: 12 filas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 466:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión 'dim_year' creada: 42 filas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "cols = clean_df.columns\n",
    "required_cols = [\"name\", \"genre\", \"platform\", \"developer\", \"publisher\", \"year\"]\n",
    "for c in required_cols:\n",
    "    if c not in cols:\n",
    "        raise ValueError(f\"Columna requerida no encontrada en el CSV: {c}\")\n",
    "\n",
    "print(\"Columnas disponibles después de normalización:\")\n",
    "print(cols)\n",
    "\n",
    "print(\"\\n===== Creación de dimensiones =====\")\n",
    "\n",
    "# Dimensión: dim_game\n",
    "dim_game_pd = (\n",
    "    clean_df.select(\"name\", \"genre\")\n",
    "    .distinct()\n",
    "    .orderBy(\"name\", \"genre\")\n",
    "    .toPandas()\n",
    ")\n",
    "dim_game_pd[\"id_game\"] = range(1, len(dim_game_pd) + 1)\n",
    "dim_game = spark.createDataFrame(dim_game_pd)\n",
    "print(f\"Dimensión 'dim_game' creada: {len(dim_game_pd)} filas\")\n",
    "\n",
    "# Dimensión: dim_platform\n",
    "dim_platform_pd = clean_df.select(\"platform\").distinct().orderBy(\"platform\").toPandas()\n",
    "dim_platform_pd[\"id_platform\"] = range(1, len(dim_platform_pd) + 1)\n",
    "dim_platform = spark.createDataFrame(dim_platform_pd)\n",
    "print(f\"Dimensión 'dim_platform' creada: {len(dim_platform_pd)} filas\")\n",
    "\n",
    "# Dimensión: dim_developer\n",
    "dim_developer_pd = clean_df.select(\"developer\").distinct().orderBy(\"developer\").toPandas()\n",
    "dim_developer_pd[\"id_developer\"] = range(1, len(dim_developer_pd) + 1)\n",
    "dim_developer = spark.createDataFrame(dim_developer_pd)\n",
    "print(f\"Dimensión 'dim_developer' creada: {len(dim_developer_pd)} filas\")\n",
    "\n",
    "# Dimensión: dim_publisher\n",
    "dim_publisher_pd = clean_df.select(\"publisher\").distinct().orderBy(\"publisher\").toPandas()\n",
    "dim_publisher_pd[\"id_publisher\"] = range(1, len(dim_publisher_pd) + 1)\n",
    "dim_publisher = spark.createDataFrame(dim_publisher_pd)\n",
    "print(f\"Dimensión 'dim_publisher' creada: {len(dim_publisher_pd)} filas\")\n",
    "\n",
    "# Dimensión: dim_year\n",
    "dim_year_pd = clean_df.select(\"year\").distinct().orderBy(\"year\").toPandas()\n",
    "dim_year_pd[\"id_year\"] = range(1, len(dim_year_pd) + 1)\n",
    "dim_year = spark.createDataFrame(dim_year_pd)\n",
    "print(f\"Dimensión 'dim_year' creada: {len(dim_year_pd)} filas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f4b88",
   "metadata": {},
   "source": [
    "# 7. Carga en SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55517c71",
   "metadata": {},
   "source": [
    "Se carga el Data Warehouse en SQLite: se convierten los DataFrames de Spark a Pandas y se usan to_sql para insertar dimensiones y hechos en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2af484a4-8e0c-4ac8-97de-a8a24f0a5208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabla de hechos 'fact_sales' creada: 10000 filas\n",
      "+-------+-----------+------------+------------+-------+--------------------+--------------------+\n",
      "|id_game|id_platform|id_developer|id_publisher|id_year|copies_sold_millions|revenue_millions_usd|\n",
      "+-------+-----------+------------+------------+-------+--------------------+--------------------+\n",
      "|    200|          3|           5|          10|     23|           1500000.0|               282.3|\n",
      "|    339|          4|          12|          10|     39|               34.55|               5.0E8|\n",
      "|    373|          1|           8|           9|     33|1.0308561989154695E7|               1.0E9|\n",
      "|     38|          7|           1|          10|     41|           1500000.0| 5.073935769315999E8|\n",
      "|    371|          6|           9|           6|     34|               12.79| 5.073935769315999E8|\n",
      "+-------+-----------+------------+------------+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# Construcción de la tabla de hechos (fact_sales)\n",
    "# -------------------------------------------------------\n",
    "fact = (\n",
    "    clean_df\n",
    "    .join(dim_game, on=[\"name\", \"genre\"], how=\"left\")\n",
    "    .join(dim_platform, on=[\"platform\"], how=\"left\")\n",
    "    .join(dim_developer, on=[\"developer\"], how=\"left\")\n",
    "    .join(dim_publisher, on=[\"publisher\"], how=\"left\")\n",
    "    .join(dim_year, on=[\"year\"], how=\"left\")\n",
    ")\n",
    "\n",
    "# Columnas métricas\n",
    "value_cols = [c for c in [\"copies_sold_millions\", \"revenue_millions_usd\"] if c in fact.columns]\n",
    "if not value_cols:\n",
    "    raise ValueError(\"No se encontraron columnas métricas para construir la tabla de hechos.\")\n",
    "\n",
    "# Selección final de la tabla de hechos\n",
    "fact_sales = fact.select(\n",
    "    \"id_game\", \"id_platform\", \"id_developer\", \"id_publisher\", \"id_year\", *value_cols\n",
    ")\n",
    "\n",
    "print(f\"\\nTabla de hechos 'fact_sales' creada: {fact_sales.count()} filas\")\n",
    "fact_sales.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0660f93",
   "metadata": {},
   "source": [
    "# 8. Consultas y Validación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f7d130",
   "metadata": {},
   "source": [
    "Se ejecuta una consulta SQL de ejemplo para validar el Data Warehouse: se calcula el top 10 de géneros por ventas usando joins entre fact_sales y dim_game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee5d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Carga de tablas en SQLite =====\n",
      "Directorio warehouse: /app/warehouse\n",
      "Base de datos: /app/warehouse/warehouse_pyspark.db\n",
      "\n",
      "Cargando tablas en SQLite...\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) unable to open database file\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py:143\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbapi_connection \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mloaded_dbapi\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py:3309\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3288\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[1;32m   3289\u001b[0m \n\u001b[1;32m   3290\u001b[0m \u001b[38;5;124;03mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3307\u001b[0m \n\u001b[1;32m   3308\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py:447\u001b[0m, in \u001b[0;36mPool.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py:1264\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[0;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[0;32m-> 1264\u001b[0m     fairy \u001b[38;5;241m=\u001b[39m \u001b[43m_ConnectionRecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py:711\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[0;34m(cls, pool)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 711\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py:178\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dec_overflow()\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py:175\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py:388\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py:673\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[0;34m(self, pool, connect)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[0;32m--> 673\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalize_callback \u001b[38;5;241m=\u001b[39m deque()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py:900\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[0;32m--> 900\u001b[0m         pool\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError on connect(): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;66;03m# the engine, so this will usually not be set\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py:895\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarttime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 895\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbapi_connection \u001b[38;5;241m=\u001b[39m connection \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m pool\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/create.py:661\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[0;34m(connection_record)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[0;32m--> 661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py:630\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[0;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcargs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DBAPIConnection:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloaded_dbapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOperationalError\u001b[0m: unable to open database file",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m engine \u001b[38;5;241m=\u001b[39m sqlalchemy\u001b[38;5;241m.\u001b[39mcreate_engine(db_url_abs)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCargando tablas en SQLite...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mbegin() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Usar directamente los DataFrames de Pandas (ya creados sin ventanas)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     dim_game_pd\u001b[38;5;241m.\u001b[39mto_sql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim_game\u001b[39m\u001b[38;5;124m\"\u001b[39m, conn, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - dim_game cargada\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py:3249\u001b[0m, in \u001b[0;36mEngine.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3224\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m   3225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbegin\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Connection]:\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a context manager delivering a :class:`_engine.Connection`\u001b[39;00m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;124;03m    with a :class:`.Transaction` established.\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3247\u001b[0m \n\u001b[1;32m   3248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m-> 3249\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m   3250\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mbegin():\n\u001b[1;32m   3251\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m conn\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py:3285\u001b[0m, in \u001b[0;36mEngine.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Connection:\n\u001b[1;32m   3263\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[1;32m   3264\u001b[0m \n\u001b[1;32m   3265\u001b[0m \u001b[38;5;124;03m    The :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3282\u001b[0m \n\u001b[1;32m   3283\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py:145\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbapi_connection \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mraw_connection()\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mloaded_dbapi\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 145\u001b[0m         \u001b[43mConnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception_noconnection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py:2448\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception_noconnection\u001b[0;34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001b[0m\n\u001b[1;32m   2446\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[1;32m   2447\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2450\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py:143\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbapi_connection \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mloaded_dbapi\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    145\u001b[0m         Connection\u001b[38;5;241m.\u001b[39m_handle_dbapi_exception_noconnection(\n\u001b[1;32m    146\u001b[0m             err, dialect, engine\n\u001b[1;32m    147\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py:3309\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraw_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PoolProxiedConnection:\n\u001b[1;32m   3288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[1;32m   3289\u001b[0m \n\u001b[1;32m   3290\u001b[0m \u001b[38;5;124;03m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3307\u001b[0m \n\u001b[1;32m   3308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py:447\u001b[0m, in \u001b[0;36mPool.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PoolProxiedConnection:\n\u001b[1;32m    440\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py:1264\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[0;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_checkout\u001b[39m(\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     fairy: Optional[_ConnectionFairy] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ConnectionFairy:\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[0;32m-> 1264\u001b[0m         fairy \u001b[38;5;241m=\u001b[39m \u001b[43m_ConnectionRecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1267\u001b[0m             threadconns\u001b[38;5;241m.\u001b[39mcurrent \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mref(fairy)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py:711\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[0;34m(cls, pool)\u001b[0m\n\u001b[1;32m    709\u001b[0m     rec \u001b[38;5;241m=\u001b[39m cast(_ConnectionRecord, pool\u001b[38;5;241m.\u001b[39m_do_get())\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 711\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     dbapi_connection \u001b[38;5;241m=\u001b[39m rec\u001b[38;5;241m.\u001b[39mget_connection()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py:178\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[0;32m--> 178\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dec_overflow()\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py:175\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inc_overflow():\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py:388\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ConnectionPoolEntry:\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py:673\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[0;34m(self, pool, connect)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pool \u001b[38;5;241m=\u001b[39m pool\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[0;32m--> 673\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalize_callback \u001b[38;5;241m=\u001b[39m deque()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py:900\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[0;32m--> 900\u001b[0m         pool\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError on connect(): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;66;03m# the engine, so this will usually not be set\u001b[39;00m\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mfirst_connect:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py:895\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarttime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbapi_connection \u001b[38;5;241m=\u001b[39m connection \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m     pool\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, connection)\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/create.py:661\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[0;34m(connection_record)\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[0;32m--> 661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py:630\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[0;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcargs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DBAPIConnection:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloaded_dbapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOperationalError\u001b[0m: (sqlite3.OperationalError) unable to open database file\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "# 9. Consultas y Validación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7ebe39",
   "metadata": {},
   "source": [
    "# 9. Resumen Final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30baafb5",
   "metadata": {},
   "source": [
    "Se resume el proceso ETL completado con PySpark: se confirma la carga exitosa, se listan las tablas creadas y se destaca el uso de Spark para procesamiento distribuido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1816ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Consulta: Top 10 géneros por ventas =====\n",
      "Consulta SQL ejecutada:\n",
      " \n",
      "SELECT \n",
      "    g.genre, \n",
      "    SUM(f.copies_sold_millions) AS total_copies_sold,\n",
      "    SUM(f.revenue_millions_usd) AS total_revenue\n",
      "FROM fact_sales f\n",
      "JOIN dim_game g ON f.id_game = g.id_game\n",
      "GROUP BY g.genre\n",
      "ORDER BY total_copies_sold DESC\n",
      "LIMIT 10;\n",
      "\n",
      "\n",
      "===== Resultado: Top 10 géneros por ventas =====\n",
      "       genre  total_copies_sold  total_revenue\n",
      "0  Adventure       14194.671731  379461.549909\n",
      "1     Puzzle       14144.021904  370930.434640\n",
      "2          ?       14010.084514  338694.744293\n",
      "3        RPG       14006.675556  368915.969530\n",
      "4     Racing       13935.575817  336646.674514\n",
      "5       RPG        13930.538948  370828.224356\n",
      "6     Sports       13580.824427  362876.689467\n",
      "7    Unknown       13575.054601  373894.664546\n",
      "8     action       13468.101123  336632.654577\n",
      "9      Indie       13277.771731  330911.439309\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de consulta sobre el Data Warehouse generado por PySpark\n",
    "\n",
    "print(\"===== Consulta: Top 10 géneros por ventas =====\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    g.genre, \n",
    "    SUM(f.copies_sold_millions) AS total_copies_sold,\n",
    "    SUM(f.revenue_millions_usd) AS total_revenue\n",
    "FROM fact_sales f\n",
    "JOIN dim_game g ON f.id_game = g.id_game\n",
    "GROUP BY g.genre\n",
    "ORDER BY total_copies_sold DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Consulta SQL ejecutada:\\n\", query)\n",
    "\n",
    "# Usar la misma conexión que creamos arriba\n",
    "with engine.connect() as conn:\n",
    "    top_genres_spark = pd.read_sql(query, conn)\n",
    "\n",
    "print(\"\\n===== Resultado: Top 10 géneros por ventas =====\")\n",
    "print(top_genres_spark)\n",
    "\n",
    "print(\"\\n===== Resumen del Data Warehouse =====\")\n",
    "print(f\"Base de datos creada en: {db_path}\")\n",
    "print(\"\\nTablas creadas:\")\n",
    "print(\"  - dim_game (dimensiones de juegos)\")\n",
    "print(\"  - dim_platform (dimensiones de plataformas)\")\n",
    "print(\"  - dim_developer (dimensiones de desarrolladores)\")\n",
    "print(\"  - dim_publisher (dimensiones de publishers)\")\n",
    "print(\"  - dim_year (dimensiones de años)\")\n",
    "print(\"  - fact_sales (tabla de hechos con ventas)\")\n",
    "print(\"\\n✅ Proceso ETL con PySpark completado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4185e",
   "metadata": {},
   "source": [
    "# 10. Resumen Final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b084b",
   "metadata": {},
   "source": [
    "Se muestra el resultado de la consulta y se proporciona un resumen completo del Data Warehouse generado, incluyendo estadísticas y confirmación de objetivos cumplidos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
