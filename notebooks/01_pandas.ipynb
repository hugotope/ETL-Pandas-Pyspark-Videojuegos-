{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299e6b18",
   "metadata": {},
   "source": [
    "# 01 - Análisis y ETL con Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1ab71",
   "metadata": {},
   "source": [
    "Este notebook realiza la exploración, limpieza y el proceso ETL del dataset de videojuegos usando Pandas y carga el resultado a una base de datos SQLite (`warehouse/warehouse_pandas.db`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0684e6",
   "metadata": {},
   "source": [
    "## 1. Configuración Inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a410d",
   "metadata": {},
   "source": [
    "Se importan las librerías necesarias para el análisis: Pandas para manipulación de datos, SQLAlchemy para conexión a bases de datos, NumPy para operaciones numéricas, y otras utilidades. Se verifica que se carguen correctamente mostrando sus versiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55306ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "# -------------------------------\n",
    "# Mensajes de información\n",
    "# -------------------------------\n",
    "\n",
    "print(\"Librerías cargadas correctamente:\")\n",
    "print(\" - os:\", os)\n",
    "print(\" - re (expresiones regulares):\", re)\n",
    "print(\" - numpy:\", np.__version__)\n",
    "print(\" - pandas:\", pd.__version__)\n",
    "print(\" - sqlalchemy:\", sqlalchemy.__version__)\n",
    "print(\"\\nPreparado para cargar y procesar el dataset de videojuegos.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c19ac2",
   "metadata": {},
   "source": [
    "## 2. Carga del Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1518ebc",
   "metadata": {},
   "source": [
    "Se carga el dataset de videojuegos desde el archivo CSV ubicado en `data/videogames.csv` usando `pd.read_csv()`. Se muestra información básica como dimensiones del dataset, tipos de datos y las primeras filas para confirmar la carga correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se carga el CSV desde la carpeta /data\n",
    "raw_df = pd.read_csv('../data/videogames.csv')\n",
    "\n",
    "# Mensaje informativo y revisión inicial\n",
    "print(\"Dataset cargado correctamente desde '../data/videogames.csv'\")\n",
    "print(f\"Dimensiones del dataset: {raw_df.shape[0]} filas x {raw_df.shape[1]} columnas\")\n",
    "print(\"Primeras 5 filas del dataset:\")\n",
    "print(raw_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9662d5a",
   "metadata": {},
   "source": [
    "## 3. Exploración Inicial del Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84af62f",
   "metadata": {},
   "source": [
    "Se realiza una exploración inicial del dataset usando `info()` y `describe()`. Se muestran estadísticas descriptivas, tipos de datos, valores únicos y presencia de nulos para entender la estructura y calidad de los datos antes de proceder con la limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspección inicial de los datos\n",
    "print(\"===== Información general del dataset =====\")\n",
    "raw_df.info() # Muestra tipos de datos, nulos y número de filas/columnas\n",
    "\n",
    "print(\"\\n===== Estadísticas descriptivas =====\")\n",
    "print(raw_df.describe(include=\"all\").T) # Muestra estadísticas descriptivas de todas las columnas (numéricas y categóricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d02a9f1",
   "metadata": {},
   "source": [
    "## 4. Limpieza de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac2e28",
   "metadata": {},
   "source": [
    "Se eliminan filas duplicadas con `drop_duplicates()` y se tratan valores faltantes: nulos en columnas numéricas (como `copies_sold_millions` y `revenue_millions_usd`) se imputan con la media después de convertir formatos especiales (ej. \"1.5M\" a números), y en categóricas se rellenan con \"Unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== 1️ Eliminación de duplicados =====\")\n",
    "clean_df = raw_df.drop_duplicates().copy()\n",
    "print(f\"Filas después de eliminar duplicados: {clean_df.shape[0]}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Identificación de tipos de columnas\n",
    "# -------------------------------\n",
    "numeric_cols = clean_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = clean_df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Limpieza de columnas numéricas problemáticas\n",
    "# --------------------------------------------------\n",
    "def parse_numeric(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    if isinstance(val, str):\n",
    "        val = val.strip().replace(\",\", \"\").replace(\"$\", \"\").upper()\n",
    "        if val in [\"UNKNOWN\", \"?\"]:\n",
    "            return np.nan\n",
    "        if val.endswith(\"M\"):\n",
    "            try: return float(val[:-1]) * 1e6\n",
    "            except: return np.nan\n",
    "        if val.endswith(\"B\"):\n",
    "            try: return float(val[:-1]) * 1e9\n",
    "            except: return np.nan\n",
    "    try:\n",
    "        return float(val)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "print(\"\\n===== 2 Limpieza de columnas de ventas/ingresos =====\")\n",
    "for col in [\"copies_sold_millions\", \"revenue_millions_usd\"]:\n",
    "    if col in clean_df.columns:\n",
    "        # Aplicamos la función de limpieza\n",
    "        clean_df[col] = clean_df[col].apply(parse_numeric)\n",
    "        # Convertimos a tipo float explícitamente\n",
    "        clean_df[col] = pd.to_numeric(clean_df[col], errors='coerce')\n",
    "        # Rellenamos los NaN con la media\n",
    "        clean_df[col] = clean_df[col].fillna(clean_df[col].mean())\n",
    "        print(f\"Columna '{col}' limpiada. Valores nulos restantes: {clean_df[col].isna().sum()}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Imputación de columnas categóricas\n",
    "# --------------------------------------------------\n",
    "cat_cols = clean_df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "print(\"\\n===== 3 Imputación de columnas categóricas =====\")\n",
    "for col in cat_cols:\n",
    "    clean_df[col] = clean_df[col].fillna(\"Unknown\")\n",
    "    print(f\"Columna '{col}' rellenada con 'Unknown'. Valores nulos restantes: {clean_df[col].isna().sum()}\")\n",
    "\n",
    "# Resumen final\n",
    "print(\"\\n===== Resumen final de limpieza =====\")\n",
    "print(clean_df.isna().sum())\n",
    "print(f\"Dimensiones del dataset limpio: {clean_df.shape[0]} filas x {clean_df.shape[1]} columnas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed897511",
   "metadata": {},
   "source": [
    "## 5. Normalización y Transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ab86c6",
   "metadata": {},
   "source": [
    "Se normalizan los nombres de las columnas a formato snake_case (minúsculas, guiones bajos) para consistencia usando operaciones de string de Pandas. Se verifican las columnas requeridas para el modelo dimensional antes de construir las tablas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== Normalización de nombres de columnas =====\")\n",
    "print(\"Columnas antes de normalizar:\")\n",
    "print(list(raw_df.columns))\n",
    "\n",
    "# Convertimos nombres a snake_case para facilidad de uso\n",
    "clean_df.columns = (\n",
    "    clean_df.columns\n",
    "    .str.strip()             # Elimina espacios al inicio y final\n",
    "    .str.lower()             # Convierte a minúsculas\n",
    "    .str.replace(\" \", \"_\", regex=False)  # Reemplaza espacios por '_'\n",
    "    .str.replace(\"-\", \"_\", regex=False)  # Reemplaza '-' por '_'\n",
    ")\n",
    "\n",
    "print(\"\\nColumnas después de normalizar a snake_case:\")\n",
    "print(list(clean_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c80b9e",
   "metadata": {},
   "source": [
    "## 6. Creación de Dimensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7aa674",
   "metadata": {},
   "source": [
    "Se crean las tablas de dimensiones del modelo en estrella: `dim_game`, `dim_platform`, `dim_developer`, `dim_publisher` y `dim_year`. Se obtienen valores únicos con `drop_duplicates()`, se ordenan y se asignan IDs autoincrementales usando NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e3557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== Validación de columnas requeridas =====\")\n",
    "required_cols = [\"name\", \"genre\", \"platform\", \"developer\", \"publisher\", \"year\"]\n",
    "for c in required_cols:\n",
    "    if c not in clean_df.columns:\n",
    "        raise ValueError(f\"Columna requerida no encontrada en el CSV: {c}\")\n",
    "print(\"Todas las columnas requeridas están presentes.\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# Dimensión: Videojuegos\n",
    "# -------------------------------\n",
    "dim_game = clean_df[[\"name\", \"genre\"]].drop_duplicates().reset_index(drop=True)\n",
    "dim_game[\"id_game\"] = np.arange(1, len(dim_game) + 1)\n",
    "print(\"Dimensión 'dim_game' creada:\")\n",
    "print(f\"Filas: {dim_game.shape[0]}\")\n",
    "print(dim_game.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Dimensión: Plataformas\n",
    "# -------------------------------\n",
    "dim_platform = clean_df[[\"platform\"]].drop_duplicates().reset_index(drop=True)\n",
    "dim_platform[\"id_platform\"] = np.arange(1, len(dim_platform) + 1)\n",
    "print(\"\\nDimensión 'dim_platform' creada:\")\n",
    "print(f\"Filas: {dim_platform.shape[0]}\")\n",
    "print(dim_platform.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Dimensión: Desarrolladores\n",
    "# -------------------------------\n",
    "dim_developer = clean_df[[\"developer\"]].drop_duplicates().reset_index(drop=True)\n",
    "dim_developer[\"id_developer\"] = np.arange(1, len(dim_developer) + 1)\n",
    "print(\"\\nDimensión 'dim_developer' creada:\")\n",
    "print(f\"Filas: {dim_developer.shape[0]}\")\n",
    "print(dim_developer.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Dimensión: Publisher\n",
    "# -------------------------------\n",
    "dim_publisher = clean_df[[\"publisher\"]].drop_duplicates().reset_index(drop=True)\n",
    "dim_publisher[\"id_publisher\"] = np.arange(1, len(dim_publisher) + 1)\n",
    "print(\"\\nDimensión 'dim_publisher' creada:\")\n",
    "print(f\"Filas: {dim_publisher.shape[0]}\")\n",
    "print(dim_publisher.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Dimensión: Año\n",
    "# -------------------------------\n",
    "dim_year = clean_df[[\"year\"]].drop_duplicates().reset_index(drop=True)\n",
    "dim_year[\"id_year\"] = np.arange(1, len(dim_year) + 1)\n",
    "print(\"\\nDimensión 'dim_year' creada:\")\n",
    "print(f\"Filas: {dim_year.shape[0]}\")\n",
    "print(dim_year.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8eeda",
   "metadata": {},
   "source": [
    "## 7. Construcción de la Tabla de Hechos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c2737",
   "metadata": {},
   "source": [
    "Se construye la tabla de hechos `fact_sales` mediante merges (joins) entre el dataset limpio y las dimensiones usando `merge()` con `how='left'`. Se incluyen las métricas `copies_sold_millions` y `revenue_millions_usd` junto con las claves foráneas de las dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8def98e-4d0c-4917-9fe6-6a3e829d4af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== Unión de dimensiones para crear la tabla de hechos =====\")\n",
    "# Se unen todas las dimensiones al dataset original para generar la tabla de hechos\n",
    "fact = clean_df.merge(dim_game, on=[\"name\", \"genre\"], how=\"left\")\n",
    "fact = fact.merge(dim_platform, on=[\"platform\"], how=\"left\")\n",
    "fact = fact.merge(dim_developer, on=[\"developer\"], how=\"left\")\n",
    "fact = fact.merge(dim_publisher, on=[\"publisher\"], how=\"left\")\n",
    "fact = fact.merge(dim_year, on=[\"year\"], how=\"left\")\n",
    "print(\"Unión completada. Dimensiones del dataset de hechos:\", fact.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# Selección de columnas métricas\n",
    "# -------------------------------\n",
    "# Verificamos qué columnas de ventas/ingresos existen en el dataset\n",
    "value_cols = [c for c in [\"copies_sold_millions\", \"revenue_millions_usd\"] if c in fact.columns]\n",
    "if not value_cols:\n",
    "    raise ValueError(\"No se encontraron columnas de ventas/ingresos en el CSV.\")\n",
    "print(\"Columnas métricas seleccionadas para la tabla de hechos:\", value_cols)\n",
    "\n",
    "# -------------------------------\n",
    "# Tabla de hechos final\n",
    "# -------------------------------\n",
    "fact_sales = fact[[\"id_game\", \"id_platform\", \"id_developer\", \"id_publisher\", \"id_year\"] + value_cols].copy()\n",
    "print(\"\\nTabla de hechos 'fact_sales' creada:\")\n",
    "print(f\"Dimensiones: {fact_sales.shape[0]} filas x {fact_sales.shape[1]} columnas\")\n",
    "print(\"Primeros registros de la tabla de hechos:\")\n",
    "print(fact_sales.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d043d50",
   "metadata": {},
   "source": [
    "## 8. Carga en SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70348820",
   "metadata": {},
   "source": [
    "Se carga el Data Warehouse en SQLite: se crea la conexión con SQLAlchemy usando `create_engine()`, se crea la carpeta `warehouse` si no existe, y se insertan las tablas de dimensiones y hechos usando `to_sql()` con `if_exists='replace'` para sobrescribir si existen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae4bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== 1 Creación de carpeta y configuración de la base de datos =====\")\n",
    "# Creamos la carpeta warehouse si no existe\n",
    "os.makedirs(\"../warehouse\", exist_ok=True)\n",
    "DB_PATH = \"../warehouse/warehouse_pandas.db\"\n",
    "DB_URL = f\"sqlite:///{DB_PATH}\"\n",
    "print(f\"Base de datos SQLite se guardará en: {DB_PATH}\")\n",
    "\n",
    "# Creamos el engine para SQLite\n",
    "engine = sqlalchemy.create_engine(DB_URL)\n",
    "\n",
    "# -------------------------------\n",
    "# Guardado de tablas dimensionales y tabla de hechos\n",
    "# -------------------------------\n",
    "print(\"\\n===== 2️ Guardado de tablas en SQLite =====\")\n",
    "with engine.begin() as conn:\n",
    "    dim_game.to_sql(\"dim_game\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - dim_game cargada\")\n",
    "    dim_platform.to_sql(\"dim_platform\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - dim_platform cargada\")\n",
    "    dim_developer.to_sql(\"dim_developer\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - dim_developer cargada\")\n",
    "    dim_publisher.to_sql(\"dim_publisher\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - dim_publisher cargada\")\n",
    "    dim_year.to_sql(\"dim_year\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - dim_year cargada\")\n",
    "    fact_sales.to_sql(\"fact_sales\", conn, if_exists=\"replace\", index=False)\n",
    "    print(\" - fact_sales cargada\")\n",
    "\n",
    "print(\"\\n Todas las tablas fueron cargadas correctamente en SQLite.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9b426d",
   "metadata": {},
   "source": [
    "## 9. Consultas y Validación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e933f08",
   "metadata": {},
   "source": [
    "Se ejecutan consultas SQL de ejemplo sobre el Data Warehouse para validar los datos: se calcula el top 10 de géneros por ventas usando `pd.read_sql()` con joins entre `fact_sales` y `dim_game`, y se muestra el resultado para confirmar que el DW funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5325b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== Consulta: Top 10 géneros por ventas =====\")\n",
    "\n",
    "# Consulta SQL para obtener los géneros con mayores ventas en millones de copias\n",
    "query = \"\"\"\n",
    "SELECT g.genre, SUM(f.copies_sold_millions) AS total_sales\n",
    "FROM fact_sales f\n",
    "JOIN dim_game g ON f.id_game = g.id_game\n",
    "GROUP BY g.genre\n",
    "ORDER BY total_sales DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(\"Consulta SQL ejecutada:\\n\", query)\n",
    "\n",
    "# Ejecutamos la consulta y la cargamos en un DataFrame de Pandas\n",
    "with engine.connect() as conn:\n",
    "    top_genres = pd.read_sql(query, conn)\n",
    "\n",
    "# Mostramos resultados\n",
    "print(\"\\n===== Resultado: Top 10 géneros por ventas =====\")\n",
    "print(top_genres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7e52f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
